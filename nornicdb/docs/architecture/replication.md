# Replication Architecture

This document describes the internal architecture of NornicDB's replication system for contributors and advanced users.

> **For user documentation, see [Clustering Guide](../user-guides/clustering.md)**

## Overview

NornicDB supports three replication modes to meet different availability and consistency requirements:

| Mode | Nodes | Consistency | Use Case |
|------|-------|-------------|----------|
| **Standalone** | 1 | N/A | Development, testing, small workloads |
| **Hot Standby** | 2 | Eventual | Simple HA, fast failover |
| **Raft Cluster** | 3-5 | Strong | Production HA, consistent reads |
| **Multi-Region** | 6+ | Configurable | Global distribution, disaster recovery |

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NORNICDB REPLICATION ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  MODE 1: HOT STANDBY (2 nodes)                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      WAL Stream      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚   Primary   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚   Standby   â”‚                         â”‚
â”‚  â”‚  (writes)   â”‚      (async/sync)    â”‚  (failover) â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                                 â”‚
â”‚  MODE 2: RAFT CLUSTER (3-5 nodes)                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   Leader    â”‚â—„â”€â”€â–ºâ”‚  Follower   â”‚â—„â”€â”€â–ºâ”‚  Follower   â”‚                        â”‚
â”‚  â”‚  (writes)   â”‚    â”‚  (reads)    â”‚    â”‚  (reads)    â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                  â”‚                  â”‚                                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                    Raft Consensus                                              â”‚
â”‚                                                                                 â”‚
â”‚  MODE 3: MULTI-REGION (Raft clusters + cross-region HA)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚      US-EAST REGION     â”‚      â”‚      EU-WEST REGION     â”‚                 â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”     â”‚      â”‚     â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”  â”‚                 â”‚
â”‚  â”‚  â”‚ L â”‚ â”‚ F â”‚ â”‚ F â”‚     â”‚ WAL  â”‚     â”‚ L â”‚ â”‚ F â”‚ â”‚ F â”‚  â”‚                 â”‚
â”‚  â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜     â”‚â—„â”€â”€â”€â”€â–ºâ”‚     â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜  â”‚                 â”‚
â”‚  â”‚     Raft Cluster A      â”‚async â”‚      Raft Cluster B    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Package Structure

```
pkg/replication/
â”œâ”€â”€ config.go           # Configuration loading and validation
â”œâ”€â”€ replicator.go       # Core Replicator interface and factory
â”œâ”€â”€ transport.go        # ClusterTransport for node-to-node communication
â”œâ”€â”€ ha_standby.go       # Hot Standby implementation
â”œâ”€â”€ raft.go             # Raft consensus implementation
â”œâ”€â”€ multi_region.go     # Multi-region coordinator
â”œâ”€â”€ wal.go              # WAL streaming primitives
â”œâ”€â”€ chaos_test.go       # Chaos testing infrastructure
â”œâ”€â”€ scenario_test.go    # E2E scenario tests
â””â”€â”€ replication_test.go # Unit tests
```

## Core Interfaces

### Replicator Interface

All replication modes implement this interface:

```go
type Replicator interface {
    // Start starts the replicator
    Start(ctx context.Context) error
    
    // Apply applies a write operation (routes to leader if needed)
    Apply(cmd *Command, timeout time.Duration) error
    
    // IsLeader returns true if this node can accept writes
    IsLeader() bool
    
    // LeaderAddr returns the address of the current leader
    LeaderAddr() string
    
    // LeaderID returns the ID of the current leader
    LeaderID() string
    
    // Health returns health status
    Health() *HealthStatus
    
    // WaitForLeader blocks until a leader is elected
    WaitForLeader(timeout time.Duration) error
    
    // Mode returns the replication mode
    Mode() ReplicationMode
    
    // NodeID returns this node's ID
    NodeID() string
    
    // Shutdown gracefully shuts down
    Shutdown() error
}
```

### Transport Interface

Node-to-node communication:

```go
type Transport interface {
    // Connect establishes a connection to a peer
    Connect(ctx context.Context, addr string) (PeerConnection, error)
    
    // Listen accepts incoming connections
    Listen(ctx context.Context, addr string, handler ConnectionHandler) error
    
    // Close shuts down the transport
    Close() error
}

type PeerConnection interface {
    // WAL streaming (Hot Standby)
    SendWALBatch(ctx context.Context, entries []*WALEntry) (*WALBatchResponse, error)
    SendHeartbeat(ctx context.Context, req *HeartbeatRequest) (*HeartbeatResponse, error)
    SendFence(ctx context.Context, req *FenceRequest) (*FenceResponse, error)
    SendPromote(ctx context.Context, req *PromoteRequest) (*PromoteResponse, error)
    
    // Raft consensus
    SendRaftVote(ctx context.Context, req *RaftVoteRequest) (*RaftVoteResponse, error)
    SendRaftAppendEntries(ctx context.Context, req *RaftAppendEntriesRequest) (*RaftAppendEntriesResponse, error)
    
    Close() error
    IsConnected() bool
}
```

### Storage Interface

Replication layer's view of storage:

```go
type Storage interface {
    // Commands
    ApplyCommand(cmd *Command) error
    
    // WAL position tracking
    GetWALPosition() (uint64, error)
    SetWALPosition(pos uint64) error
    
    // Node/Edge operations (used by WAL applier)
    CreateNode(node *Node) error
    UpdateNode(node *Node) error
    DeleteNode(id NodeID) error
    CreateEdge(edge *Edge) error
    DeleteEdge(from, to NodeID, relType string) error
    SetProperty(nodeID NodeID, key string, value interface{}) error
}
```

## Network Protocol

### Port Allocation

| Port | Protocol | Purpose |
|------|----------|---------|
| 7474 | HTTP | REST API, Admin, Health checks |
| 7687 | Bolt | Neo4j-compatible client queries |
| 7688 | Cluster | Replication, Raft consensus |

### Wire Format

The cluster protocol uses length-prefixed JSON over TCP:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Length (4B) â”‚          JSON Payload               â”‚
â”‚ Big Endian  â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Message Types

| Type | Code | Direction | Description |
|------|------|-----------|-------------|
| VoteRequest | 1 | Candidate â†’ Follower | Request vote in election |
| VoteResponse | 2 | Follower â†’ Candidate | Grant/deny vote |
| AppendEntries | 3 | Leader â†’ Follower | Replicate log entries |
| AppendEntriesResponse | 4 | Follower â†’ Leader | Acknowledge entries |
| WALBatch | 5 | Primary â†’ Standby | Stream WAL entries |
| WALBatchResponse | 6 | Standby â†’ Primary | Acknowledge WAL |
| Heartbeat | 7 | Primary â†’ Standby | Health check |
| HeartbeatResponse | 8 | Standby â†’ Primary | Health status |
| Fence | 9 | Standby â†’ Primary | Fence old primary |
| FenceResponse | 10 | Primary â†’ Standby | Acknowledge fence |
| Promote | 11 | Admin â†’ Standby | Promote to primary |
| PromoteResponse | 12 | Standby â†’ Admin | Promotion status |

## Mode 1: Hot Standby

### Components

- **Primary**: Accepts writes, streams WAL to standby
- **Standby**: Receives WAL, ready for failover
- **WALStreamer**: Manages WAL position and batching
- **WALApplier**: Applies WAL entries to storage

### Write Flow

```
Client                  Primary                 Standby
  â”‚                        â”‚                        â”‚
  â”‚â”€â”€â”€ WRITE (Bolt) â”€â”€â”€â”€â”€â”€â”€â–º                        â”‚
  â”‚                        â”‚                        â”‚
  â”‚                        â”‚â”€â”€ WALBatch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                        â”‚                        â”‚
  â”‚                        â”‚â—„â”€ WALBatchResponse â”€â”€â”€â”€â”‚
  â”‚                        â”‚                        â”‚
  â”‚â—„â”€â”€ SUCCESS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                        â”‚
```

### Sync Modes

| Mode | Acknowledgment | Data Safety | Latency |
|------|----------------|-------------|---------|
| `async` | Primary only | Risk of data loss | Lowest |
| `semi_sync` | Standby received | Minimal data loss | Medium |
| `sync` | Standby persisted | No data loss | Highest |

### Failover Process

1. Standby detects missing heartbeats
2. After `FAILOVER_TIMEOUT`, standby attempts to fence primary
3. Standby promotes itself to primary
4. Clients reconnect to new primary

## Mode 2: Raft Consensus

### Components

- **RaftReplicator**: Main Raft node implementation
- **Election Timer**: Triggers leader election on timeout
- **Log**: In-memory Raft log with commit tracking
- **Heartbeat Loop**: Leader sends heartbeats to maintain authority

### State Machine

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Follower  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                       â”‚ election timeout       â”‚
                       â–¼                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
          â”Œâ”€â”€â”€â”€â”€â–ºâ”‚ Candidate  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ loses election  â”‚
          â”‚            â”‚                        â”‚
          â”‚            â”‚ wins election          â”‚
          â”‚            â–¼                        â”‚
          â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
          â”‚      â”‚   Leader   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ discovers higher term
          â”‚            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           starts new election
```

### Leader Election

1. Follower's election timer expires
2. Increments term, transitions to Candidate
3. Votes for self, requests votes from peers
4. If majority votes received â†’ becomes Leader
5. Sends heartbeats to maintain leadership

### Log Replication

1. Client sends write to Leader
2. Leader appends entry to log
3. Leader sends AppendEntries to all followers
4. When majority acknowledge â†’ entry committed
5. Leader applies to state machine, responds to client

### Raft RPC Messages

**RequestVote:**
```go
type RaftVoteRequest struct {
    Term         uint64
    CandidateID  string
    LastLogIndex uint64
    LastLogTerm  uint64
}

type RaftVoteResponse struct {
    Term        uint64
    VoteGranted bool
    VoterID     string
}
```

**AppendEntries:**
```go
type RaftAppendEntriesRequest struct {
    Term         uint64
    LeaderID     string
    LeaderAddr   string
    PrevLogIndex uint64
    PrevLogTerm  uint64
    Entries      []*RaftLogEntry
    LeaderCommit uint64
}

type RaftAppendEntriesResponse struct {
    Term          uint64
    Success       bool
    MatchIndex    uint64
    ConflictIndex uint64
    ConflictTerm  uint64
}
```

## Mode 3: Multi-Region

### Components

- **MultiRegionReplicator**: Coordinates local Raft + cross-region
- **Local Raft Cluster**: Strong consistency within region
- **Cross-Region Streamer**: Async WAL replication between regions

### Write Flow

1. Write arrives at region's Raft leader
2. Raft commits locally (strong consistency)
3. Async replication to remote regions
4. Remote regions apply WAL entries

### Conflict Resolution

When async replication causes conflicts:

| Strategy | Description |
|----------|-------------|
| `last_write_wins` | Latest timestamp wins |
| `first_write_wins` | Earliest timestamp wins |
| `manual` | Flag for manual resolution |

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `NORNICDB_CLUSTER_MODE` | `standalone` | `standalone`, `ha_standby`, `raft`, `multi_region` |
| `NORNICDB_CLUSTER_NODE_ID` | auto | Unique node identifier |
| `NORNICDB_CLUSTER_BIND_ADDR` | `0.0.0.0:7688` | Cluster port binding |
| `NORNICDB_CLUSTER_ADVERTISE_ADDR` | same as bind | Address advertised to peers |

See [Clustering Guide](../user-guides/clustering.md) for complete configuration reference.

## Testing

### Test Categories

| File | Purpose |
|------|---------|
| `replication_test.go` | Unit tests for each component |
| `scenario_test.go` | E2E tests for all modes (A/B/C/D scenarios) |
| `chaos_test.go` | Network failure simulation |

### Chaos Testing

The chaos testing infrastructure simulates:

- Packet loss
- High latency (2000ms+)
- Connection drops
- Data corruption
- Packet duplication
- Packet reordering
- Byzantine failures

### Running Tests

```bash
# All replication tests
go test ./pkg/replication/... -v

# Specific test
go test ./pkg/replication/... -run TestScenario_Raft -v

# With race detection
go test ./pkg/replication/... -race

# Skip long-running tests
go test ./pkg/replication/... -short
```

## Implementation Timeline

| Component | Effort | Status |
|-----------|--------|--------|
| Hot Standby | 5-7 weeks | âœ… Complete |
| Raft Cluster | 8-10 weeks | âœ… Complete |
| Multi-Region | 6-8 weeks | ğŸš§ In Progress |

## See Also

- [Clustering Guide](../user-guides/clustering.md) - User documentation
- [System Design](./system-design.md) - Overall architecture
- [Plugin System](./plugin-system.md) - APOC plugin architecture
