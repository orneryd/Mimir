services:
  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j_db
    ports:
      - "7474:7474"  # HTTP Browser UI
      - "7687:7687"  # Bolt protocol
    volumes:
      - ./data/neo4j:/data
      - ./logs/neo4j:/logs
      - ./data/neo4j/import:/var/lib/neo4j/import
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password}
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_heap_initial__size=512M
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_PLUGINS=["apoc"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p $${NEO4J_PASSWORD:-password} 'RETURN 1' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mcp_network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama_server
    ports:
      - "11434:11434"  # Ollama API
    volumes:
      - ./data/ollama:/root/.ollama  # Persist models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - mcp_network
    # Uncomment if you have GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    profiles:
      - ollama  # Only start if explicitly enabled or embeddings feature is active

  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: mcp-server:latest
    container_name: mcp_server
    restart: unless-stopped
    environment:
      # Database Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      
      # Server Configuration
      - NODE_ENV=production
      - PORT=3000
      
      # File Watching
      - FILE_WATCH_POLLING=true
      - FILE_WATCH_INTERVAL=1000
      - WORKSPACE_ROOT=/workspace
      
      # LLM Configuration (Ollama)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      
      # Feature Flags (override .mimir/llm-config.json)
      - MIMIR_FEATURE_PM_MODEL_SUGGESTIONS=${MIMIR_FEATURE_PM_MODEL_SUGGESTIONS:-false}
      - MIMIR_FEATURE_VECTOR_EMBEDDINGS=${MIMIR_FEATURE_VECTOR_EMBEDDINGS:-false}
      
      # Embeddings Configuration (only used if VECTOR_EMBEDDINGS=true)
      - MIMIR_EMBEDDINGS_ENABLED=${MIMIR_EMBEDDINGS_ENABLED:-false}
      - MIMIR_EMBEDDINGS_MODEL=${MIMIR_EMBEDDINGS_MODEL:-nomic-embed-text}
      - MIMIR_EMBEDDINGS_DIMENSIONS=${MIMIR_EMBEDDINGS_DIMENSIONS:-768}
      - MIMIR_EMBEDDINGS_CHUNK_SIZE=${MIMIR_EMBEDDINGS_CHUNK_SIZE:-512}
      - MIMIR_EMBEDDINGS_CHUNK_OVERLAP=${MIMIR_EMBEDDINGS_CHUNK_OVERLAP:-50}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ${HOST_WORKSPACE_ROOT:-~/src}:/workspace:ro
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      neo4j:
        condition: service_healthy
      # Ollama is optional - only required if embeddings enabled
      # Start with: docker-compose --profile ollama up -d
      # Or set MIMIR_FEATURE_VECTOR_EMBEDDINGS=true in .env
    networks:
      - mcp_network

networks:
  mcp_network:
    driver: bridge
